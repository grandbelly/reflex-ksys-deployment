"""
Hybrid Training Pipeline - Best of Both Worlds

ÌÜµÌï© Í∞úÏÑ†Ïïà:
1. Ï†ú ÏÑ§Í≥ÑÏùò ÌÉÄÏûÖ ÏïàÏ†ïÏÑ± + ÌîåÎü¨Í∑∏Ïù∏ ÏãúÏä§ÌÖú
2. Ï†úÏïàÌïòÏã† Ï≤¥Ïù∏ Ìå®ÌÑ¥ + Ï†ÑÏ≤òÎ¶¨/ÌõÑÏ≤òÎ¶¨
3. Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ï∂îÏ†Å
4. Î†àÏßÄÏä§Ìä∏Î¶¨ Ìå®ÌÑ¥ÏúºÎ°ú ÎèôÏ†Å ÌôïÏû•

Example:
    # Method chaining with preprocessing
    pipeline = (TrainingPipelineV2(session)
        .set_data_source("INLET_PRESSURE", days=30)
        .add_preprocessing()
            .interpolate(method='linear')
            .remove_outliers(threshold=3.0)
            .scale(method='standard')
        .done()
        .add_feature_engineering()
            .add_lag([1, 6, 24])
            .add_rolling([6, 24])
            .add_temporal(['hour', 'dayofweek'])
        .done()
        .add_model("auto_arima", seasonal=True)
        .add_model("prophet")
        .add_validation("walk_forward", n_splits=5)
        .build()
    )

    result = await pipeline.execute()
    print(result.metadata)  # Full pipeline metadata
"""

from abc import ABC, abstractmethod
from typing import Dict, List, Optional, Any, Callable
from datetime import datetime, timedelta
from dataclasses import dataclass, field
import pandas as pd
import numpy as np
import pytz
from reflex.utils import console
from sqlalchemy.ext.asyncio import AsyncSession

from ksys_app.services.feature_engineering_service import FeatureEngineeringService


KST = pytz.timezone('Asia/Seoul')


# ============================================================================
# Pipeline Metadata (Ï∂îÏ†Å Î∞è ÎîîÎ≤ÑÍπÖÏö©)
# ============================================================================

@dataclass
class PipelineMetadata:
    """ÌååÏù¥ÌîÑÎùºÏù∏ Ïã§Ìñâ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞"""

    # Data info
    tag_name: str = ""
    data_start: Optional[datetime] = None
    data_end: Optional[datetime] = None
    raw_samples: int = 0
    processed_samples: int = 0

    # Preprocessing steps
    preprocessing_steps: List[str] = field(default_factory=list)
    outliers_removed: int = 0
    interpolated_gaps: int = 0

    # Detailed preprocessing statistics
    preprocessing_details: Dict[str, Any] = field(default_factory=dict)
    # Example: {
    #   "outliers": {"removed": 10, "percentage": 1.5, "threshold": 3.0, "method": "z-score"},
    #   "interpolation": {"gaps_filled": 5, "method": "linear", "largest_gap": 3},
    # }

    # Feature engineering
    features_created: List[str] = field(default_factory=list)
    original_features: int = 0
    final_features: int = 0

    # Model training
    models_trained: List[str] = field(default_factory=list)
    training_duration: float = 0.0

    # Results
    best_model: str = ""
    best_mape: float = 0.0

    def to_dict(self) -> Dict:
        """Convert to dictionary for JSON serialization"""
        return {
            'tag_name': self.tag_name,
            'data_start': self.data_start.isoformat() if self.data_start else None,
            'data_end': self.data_end.isoformat() if self.data_end else None,
            'raw_samples': self.raw_samples,
            'processed_samples': self.processed_samples,
            'preprocessing_steps': self.preprocessing_steps,
            'outliers_removed': self.outliers_removed,
            'features_created': self.features_created,
            'models_trained': self.models_trained,
            'best_model': self.best_model,
            'best_mape': self.best_mape
        }


# ============================================================================
# Preprocessing Chain (Ï†úÏïàÌïòÏã† ÏÑ§Í≥ÑÏùò PreprocessorChain)
# ============================================================================

class PreprocessingStep(ABC):
    """Ï†ÑÏ≤òÎ¶¨ Îã®Í≥Ñ Î≤†Ïù¥Ïä§ ÌÅ¥ÎûòÏä§"""

    @abstractmethod
    async def apply(self, df: pd.DataFrame, metadata: PipelineMetadata) -> pd.DataFrame:
        """Ï†ÑÏ≤òÎ¶¨ Ï†ÅÏö©"""
        pass

    @abstractmethod
    def get_name(self) -> str:
        """Îã®Í≥Ñ Ïù¥Î¶Ñ"""
        pass


class InterpolateStep(PreprocessingStep):
    """Í≤∞Ï∏°Ïπò Î≥¥Í∞Ñ"""

    def __init__(self, method: str = 'linear'):
        self.method = method

    async def apply(self, df: pd.DataFrame, metadata: PipelineMetadata) -> pd.DataFrame:
        before_count = df['value'].isna().sum()

        # Find gap sizes before interpolation
        gaps = df['value'].isna()
        gap_sizes = []
        current_gap = 0
        for is_gap in gaps:
            if is_gap:
                current_gap += 1
            else:
                if current_gap > 0:
                    gap_sizes.append(current_gap)
                current_gap = 0
        if current_gap > 0:
            gap_sizes.append(current_gap)

        # Perform interpolation
        df['value'] = df['value'].interpolate(method=self.method)
        after_count = df['value'].isna().sum()

        filled_count = before_count - after_count
        metadata.interpolated_gaps = filled_count
        metadata.preprocessing_steps.append(f"interpolate_{self.method}")

        # Store detailed statistics
        metadata.preprocessing_details['interpolation'] = {
            "gaps_filled": filled_count,
            "method": self.method,
            "total_gaps": len(gap_sizes),
            "largest_gap": max(gap_sizes) if gap_sizes else 0,
            "average_gap_size": sum(gap_sizes) / len(gap_sizes) if gap_sizes else 0,
            "percentage": (filled_count / len(df) * 100) if len(df) > 0 else 0,
        }

        console.info(f"   ‚úì Interpolated {filled_count} gaps using {self.method} (largest: {metadata.preprocessing_details['interpolation']['largest_gap']})")
        return df

    def get_name(self) -> str:
        return f"interpolate_{self.method}"


class RemoveOutliersStep(PreprocessingStep):
    """Ïù¥ÏÉÅÏπò Ï†úÍ±∞ (Z-score Í∏∞Î∞ò)"""

    def __init__(self, threshold: float = 3.0):
        self.threshold = threshold

    async def apply(self, df: pd.DataFrame, metadata: PipelineMetadata) -> pd.DataFrame:
        # Calculate statistics before outlier removal
        mean_before = df['value'].mean()
        std_before = df['value'].std()

        z_scores = np.abs((df['value'] - mean_before) / std_before)
        before = len(df)
        df = df[z_scores < self.threshold].copy()
        after = len(df)

        removed_count = before - after
        metadata.outliers_removed = removed_count
        metadata.preprocessing_steps.append(f"remove_outliers_{self.threshold}")

        # Store detailed statistics
        metadata.preprocessing_details['outliers'] = {
            "removed": removed_count,
            "percentage": (removed_count / before * 100) if before > 0 else 0,
            "threshold": self.threshold,
            "method": "z-score",
            "mean_before": float(mean_before),
            "std_before": float(std_before),
            "mean_after": float(df['value'].mean()),
            "std_after": float(df['value'].std()),
        }

        console.info(f"   ‚úì Removed {removed_count} outliers ({metadata.preprocessing_details['outliers']['percentage']:.2f}%)")
        return df

    def get_name(self) -> str:
        return f"remove_outliers_{self.threshold}"


class ScaleStep(PreprocessingStep):
    """Ïä§ÏºÄÏùºÎßÅ"""

    def __init__(self, method: str = 'standard'):
        self.method = method

    async def apply(self, df: pd.DataFrame, metadata: PipelineMetadata) -> pd.DataFrame:
        if self.method == 'standard':
            df['value_scaled'] = (df['value'] - df['value'].mean()) / df['value'].std()
        elif self.method == 'minmax':
            df['value_scaled'] = (df['value'] - df['value'].min()) / (df['value'].max() - df['value'].min())

        metadata.preprocessing_steps.append(f"scale_{self.method}")
        console.info(f"   ‚úì Scaled with {self.method}")
        return df

    def get_name(self) -> str:
        return f"scale_{self.method}"


class PreprocessingChain:
    """Ï†ÑÏ≤òÎ¶¨ Ï≤¥Ïù∏ ÎπåÎçî"""

    def __init__(self, pipeline: 'TrainingPipelineV2'):
        self.pipeline = pipeline
        self.steps: List[PreprocessingStep] = []

    def interpolate(self, method: str = 'linear') -> 'PreprocessingChain':
        self.steps.append(InterpolateStep(method))
        return self

    def remove_outliers(self, threshold: float = 3.0) -> 'PreprocessingChain':
        self.steps.append(RemoveOutliersStep(threshold))
        return self

    def scale(self, method: str = 'standard') -> 'PreprocessingChain':
        self.steps.append(ScaleStep(method))
        return self

    def done(self) -> 'TrainingPipelineV2':
        """Ï≤¥Ïù∏ ÏôÑÎ£å - ÌååÏù¥ÌîÑÎùºÏù∏ÏúºÎ°ú Î≥µÍ∑Ä"""
        self.pipeline._preprocessing_steps = self.steps
        return self.pipeline


# ============================================================================
# Feature Engineering Chain
# ============================================================================

class FeatureEngineeringChain:
    """ÌîºÏ≤ò ÏóîÏßÄÎãàÏñ¥ÎßÅ Ï≤¥Ïù∏ ÎπåÎçî"""

    def __init__(self, pipeline: 'TrainingPipelineV2'):
        self.pipeline = pipeline
        self.config = {}

    def add_lag(self, periods: List[int]) -> 'FeatureEngineeringChain':
        self.config['lags'] = periods
        return self

    def add_rolling(self, windows: List[int]) -> 'FeatureEngineeringChain':
        self.config['rolling'] = windows
        return self

    def add_temporal(self, components: List[str]) -> 'FeatureEngineeringChain':
        self.config['temporal'] = components
        return self

    def add_fourier(self, periods: List[int]) -> 'FeatureEngineeringChain':
        self.config['fourier'] = periods
        return self

    def done(self) -> 'TrainingPipelineV2':
        """Ï≤¥Ïù∏ ÏôÑÎ£å - ÌååÏù¥ÌîÑÎùºÏù∏ÏúºÎ°ú Î≥µÍ∑Ä"""
        self.pipeline._feature_config = self.config
        return self.pipeline


# ============================================================================
# Model Plugin (Í∏∞Ï°¥ ÏÑ§Í≥Ñ Ïú†ÏßÄ)
# ============================================================================

class ModelPlugin(ABC):
    """Î™®Îç∏ ÌîåÎü¨Í∑∏Ïù∏ Î≤†Ïù¥Ïä§ ÌÅ¥ÎûòÏä§"""

    def __init__(self, **params):
        self.params = params
        self.model = None

    @abstractmethod
    async def train(self, data: pd.DataFrame) -> Any:
        pass

    @abstractmethod
    async def predict(self, horizon: int) -> pd.DataFrame:
        pass

    @abstractmethod
    def get_name(self) -> str:
        pass


class AutoARIMAPlugin(ModelPlugin):
    """Auto ARIMA model plugin using statsforecast with diagnostics"""

    def __init__(self, **params):
        super().__init__(**params)
        self.training_data = None
        self.model_info = {}  # Store ARIMA parameters, residuals, etc.

    def get_name(self) -> str:
        return "auto_arima"

    async def train(self, data: pd.DataFrame) -> Any:
        """Train Auto ARIMA model with full diagnostics"""
        try:
            from statsforecast import StatsForecast
            from statsforecast.models import AutoARIMA
            from statsforecast.arima import arima_string

            # Prepare data for statsforecast
            df = data.copy()
            df['ds'] = df['timestamp']
            df['y'] = df['value']
            df['unique_id'] = 'series_1'

            # Store training data for later use
            self.training_data = df[['unique_id', 'ds', 'y']].copy()

            # Create model with season_length based on data frequency
            season_length = self.params.get('season_length', 24)  # Default: hourly data with daily seasonality
            models = [AutoARIMA(season_length=season_length)]
            sf = StatsForecast(models=models, freq='H', n_jobs=1)  # Use n_jobs=1 for single time series

            # Fit model
            console.info(f"Training {self.get_name()} with season_length={season_length}...")
            sf.fit(self.training_data)

            self.model = sf

            # Extract model diagnostics
            try:
                fitted_model = sf.fitted_[0, 0].model_

                # Get ARIMA string representation (e.g., "ARIMA(4,0,3)(0,1,1)[12]")
                arima_params = arima_string(fitted_model)
                self.model_info['arima_string'] = arima_params.strip()

                # Get model parameters
                self.model_info['arma'] = fitted_model.get('arma', None)
                self.model_info['aic'] = fitted_model.get('aic', None)
                self.model_info['bic'] = fitted_model.get('bic', None)
                self.model_info['aicc'] = fitted_model.get('aicc', None)
                self.model_info['sigma2'] = fitted_model.get('sigma2', None)

                # Get residuals for diagnostic plots
                residuals = fitted_model.get('residuals', None)
                if residuals is not None:
                    self.model_info['residuals'] = residuals
                    self.model_info['residuals_mean'] = float(np.mean(residuals))
                    self.model_info['residuals_std'] = float(np.std(residuals))

                console.info(f"‚úÖ {self.get_name()} trained: {self.model_info['arima_string']}")
                console.info(f"   AIC: {self.model_info['aic']:.2f}, BIC: {self.model_info['bic']:.2f}")

            except Exception as e:
                console.warn(f"Could not extract model diagnostics: {e}")

            return self.model

        except ImportError:
            console.error("statsforecast not installed")
            raise
        except Exception as e:
            console.error(f"Error training AutoARIMA: {e}")
            raise

    async def predict(self, horizon: int, level: list = None) -> pd.DataFrame:
        """
        Generate predictions with confidence intervals

        Args:
            horizon: Number of periods to forecast
            level: List of confidence levels (e.g., [80, 95])
        """
        if self.model is None:
            raise ValueError("Model not trained")

        # Generate forecast with confidence intervals
        if level:
            forecast = self.model.predict(h=horizon, level=level)
            console.info(f"Forecast generated with {len(level)} confidence intervals")
        else:
            forecast = self.model.predict(h=horizon)

        return forecast

    def get_fitted_values(self) -> pd.DataFrame:
        """Get fitted values from training"""
        if self.model is None:
            raise ValueError("Model not trained")

        try:
            fitted = self.model.forecast_fitted_values()
            return fitted
        except Exception as e:
            console.warn(f"Could not get fitted values: {e}")
            return None

    async def evaluate(self, test_data: pd.DataFrame = None, n_windows: int = 3) -> dict:
        """
        Evaluate model performance with walk-forward validation

        Args:
            test_data: Test dataset (optional, uses training data if not provided)
            n_windows: Number of walk-forward windows

        Returns:
            dict with MAE, MAPE, MASE, RMSE, SMAPE
        """
        if self.model is None:
            raise ValueError("Model not trained")

        try:
            # Use training data if test data not provided
            data = test_data if test_data is not None else self.training_data

            if data is None or len(data) == 0:
                return {}

            # Perform cross-validation with walk-forward
            horizon = min(24, len(data) // (n_windows + 1))  # Forecast 24h or less

            console.info(f"Evaluating with {n_windows} windows, horizon={horizon}")

            cv_results = self.model.cross_validation(
                df=data,
                h=horizon,
                step_size=horizon,
                n_windows=n_windows
            )

            # Calculate metrics
            actual = cv_results['y'].values
            predicted = cv_results['AutoARIMA'].values

            # MAE - Mean Absolute Error
            mae = float(np.mean(np.abs(actual - predicted)))

            # MAPE - Mean Absolute Percentage Error
            mape = float(np.mean(np.abs((actual - predicted) / actual)) * 100)

            # RMSE - Root Mean Squared Error
            rmse = float(np.sqrt(np.mean((actual - predicted) ** 2)))

            # SMAPE - Symmetric Mean Absolute Percentage Error
            smape = float(np.mean(2 * np.abs(actual - predicted) / (np.abs(actual) + np.abs(predicted))) * 100)

            # MASE - Mean Absolute Scaled Error (using naive forecast as baseline)
            naive_error = np.mean(np.abs(np.diff(actual)))
            mase = float(mae / naive_error) if naive_error > 0 else 0.0

            metrics = {
                "mae": mae,
                "mape": mape,
                "mase": mase,
                "rmse": rmse,
                "smape": smape,
                "n_windows": n_windows,
                "horizon": horizon,
                "n_predictions": len(actual),
            }

            console.info(f"   MAE: {mae:.2f}, MAPE: {mape:.2f}%, RMSE: {rmse:.2f}")

            return metrics

        except Exception as e:
            console.warn(f"Could not evaluate model: {e}")
            return {}

    def get_residuals_analysis(self) -> dict:
        """
        Analyze residuals for diagnostic purposes
        
        Returns:
            dict with residuals statistics and histogram data
        """
        if self.model is None or 'residuals' not in self.model_info:
            return {}
        
        try:
            residuals = self.model_info['residuals']
            
            # Basic statistics
            residuals_mean = float(np.mean(residuals))
            residuals_std = float(np.std(residuals))
            residuals_min = float(np.min(residuals))
            residuals_max = float(np.max(residuals))
            
            # Normality tests
            from scipy import stats
            skewness = float(stats.skew(residuals))
            kurtosis = float(stats.kurtosis(residuals))
            
            # Q-Q plot data (quantiles)
            theoretical_quantiles = stats.norm.ppf(np.linspace(0.01, 0.99, 50))
            sample_quantiles = np.percentile(residuals, np.linspace(1, 99, 50))
            
            # Histogram data (bins)
            hist_counts, hist_edges = np.histogram(residuals, bins=20)
            histogram_data = [
                {
                    "bin_start": float(hist_edges[i]),
                    "bin_end": float(hist_edges[i+1]),
                    "count": int(hist_counts[i]),
                    "bin_center": float((hist_edges[i] + hist_edges[i+1]) / 2)
                }
                for i in range(len(hist_counts))
            ]
            
            # ACF data (autocorrelation function)
            from statsmodels.tsa.stattools import acf
            acf_values = acf(residuals, nlags=24, fft=False)
            acf_data = [
                {"lag": i, "acf": float(acf_values[i])}
                for i in range(len(acf_values))
            ]
            
            analysis = {
                "statistics": {
                    "mean": residuals_mean,
                    "std": residuals_std,
                    "min": residuals_min,
                    "max": residuals_max,
                    "skewness": skewness,
                    "kurtosis": kurtosis,
                },
                "qq_plot": {
                    "theoretical": theoretical_quantiles.tolist(),
                    "sample": sample_quantiles.tolist(),
                },
                "histogram": histogram_data,
                "acf": acf_data,
            }
            
            console.info(f"Residuals analysis: mean={residuals_mean:.4f}, std={residuals_std:.4f}")
            
            return analysis
            
        except Exception as e:
            console.warn(f"Could not analyze residuals: {e}")
            return {}

    def get_diagnostics(self) -> dict:
        """Return model diagnostics for visualization"""
        return self.model_info


class ProphetPlugin(ModelPlugin):
    """Prophet model plugin"""

    def get_name(self) -> str:
        return "prophet"

    async def train(self, data: pd.DataFrame) -> Any:
        """Train Prophet model"""
        try:
            from prophet import Prophet

            # Prepare data
            df = data.copy()
            df['ds'] = df['timestamp']
            df['y'] = df['value']

            # Create and fit model
            model = Prophet(**self.params)
            model.fit(df[['ds', 'y']])

            self.model = model
            console.info(f"‚úÖ {self.get_name()} trained successfully")
            return self.model

        except ImportError:
            console.error("prophet not installed")
            raise
        except Exception as e:
            console.error(f"Error training Prophet: {e}")
            raise

    async def predict(self, horizon: int, level: list = None) -> pd.DataFrame:
        """Generate predictions with confidence intervals
        
        Args:
            horizon: Number of periods to forecast
            level: List of confidence levels (ignored for Prophet, uses default 80%)
        """
        if self.model is None:
            raise ValueError("Model not trained")

        future = self.model.make_future_dataframe(periods=horizon, freq='H')
        forecast = self.model.predict(future)
        
        # Prophet uses yhat_lower/yhat_upper for 80% confidence by default
        result = forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail(horizon).copy()
        
        # Rename to match expected format
        result = result.rename(columns={
            'yhat': 'Prophet',
            'yhat_lower': 'Prophet-lo-80',
            'yhat_upper': 'Prophet-hi-80'
        })
        
        # For 95% confidence, use wider interval (approximate)
        result['Prophet-lo-95'] = result['Prophet-lo-80'] * 1.2 - result['Prophet'] * 0.2
        result['Prophet-hi-95'] = result['Prophet-hi-80'] * 1.2 - result['Prophet'] * 0.2
        
        return result


class XGBoostPlugin(ModelPlugin):
    """XGBoost model plugin with feature engineering"""

    def get_name(self) -> str:
        return "xgboost"

    async def train(self, data: pd.DataFrame) -> Any:
        """Train XGBoost model"""
        try:
            import xgboost as xgb

            # Prepare features (assume feature engineering already done)
            feature_cols = [c for c in data.columns if c not in ['timestamp', 'value', 'tag_name']]

            if not feature_cols:
                raise ValueError("No features available for XGBoost")

            X = data[feature_cols].fillna(0)
            y = data['value'].values

            # Default params
            params = {
                'objective': 'reg:squarederror',
                'max_depth': 6,
                'learning_rate': 0.1,
                'n_estimators': 100,
                'random_state': 42,
                'n_jobs': -1
            }
            params.update(self.params)

            # Train
            model = xgb.XGBRegressor(**params)
            model.fit(X, y)

            self.model = model
            self.feature_cols = feature_cols
            console.info(f"‚úÖ {self.get_name()} trained successfully")
            return self.model

        except ImportError:
            console.error("xgboost not installed")
            raise
        except Exception as e:
            console.error(f"Error training XGBoost: {e}")
            raise

    async def predict(self, horizon: int) -> pd.DataFrame:
        """Generate predictions (requires future features)"""
        if self.model is None:
            raise ValueError("Model not trained")

        # Note: For XGBoost, you need to provide future features
        raise NotImplementedError("XGBoost prediction requires future feature generation")


# ============================================================================
# Model Registry (Ï†úÏïàÌïòÏã† ÏÑ§Í≥ÑÏùò Î†àÏßÄÏä§Ìä∏Î¶¨ Ìå®ÌÑ¥)
# ============================================================================

class ModelRegistry:
    """Î™®Îç∏ Î†àÏßÄÏä§Ìä∏Î¶¨ - ÎèôÏ†Å ÏïåÍ≥†Î¶¨Ï¶ò Ï∂îÍ∞Ä"""

    _models: Dict[str, type] = {}

    @classmethod
    def register(cls, name: str, plugin_class: type):
        """ÏÉàÎ°úÏö¥ Î™®Îç∏ Îì±Î°ù"""
        if not issubclass(plugin_class, ModelPlugin):
            raise TypeError("Must inherit from ModelPlugin")

        cls._models[name] = plugin_class
        console.info(f"üì¶ Registered model: {name}")

    @classmethod
    def get(cls, name: str) -> type:
        """Î™®Îç∏ ÌÅ¥ÎûòÏä§ Í∞ÄÏ†∏Ïò§Í∏∞"""
        if name not in cls._models:
            raise ValueError(f"Model '{name}' not registered")
        return cls._models[name]

    @classmethod
    def list_models(cls) -> List[str]:
        """Îì±Î°ùÎêú Î™®Îç∏ Î™©Î°ù"""
        return list(cls._models.keys())


# Default models registration
ModelRegistry.register('auto_arima', AutoARIMAPlugin)
ModelRegistry.register('prophet', ProphetPlugin)
ModelRegistry.register('xgboost', XGBoostPlugin)


# ============================================================================
# Hybrid Training Pipeline V2
# ============================================================================

class TrainingPipelineV2:
    """
    ÌïòÏù¥Î∏åÎ¶¨Îìú ÌïôÏäµ ÌååÏù¥ÌîÑÎùºÏù∏

    Features:
    - Î©îÏÑúÎìú Ï≤¥Ïù¥Îãù (fluent API)
    - Ï†ÑÏ≤òÎ¶¨/ÌõÑÏ≤òÎ¶¨ Ï≤¥Ïù∏
    - Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ï∂îÏ†Å
    - Î†àÏßÄÏä§Ìä∏Î¶¨ Ìå®ÌÑ¥
    - ÌÉÄÏûÖ ÏïàÏ†ïÏÑ±
    """

    def __init__(self, session: AsyncSession):
        self.session = session
        self.feature_service = FeatureEngineeringService(session)

        # Configuration
        self._data_source = {}
        self._preprocessing_steps: List[PreprocessingStep] = []
        self._feature_config = {}
        self._models: List[ModelPlugin] = []
        self._validation = None

        # Data cache
        self._raw_data: Optional[pd.DataFrame] = None
        self._processed_data: Optional[pd.DataFrame] = None

        # Metadata
        self.metadata = PipelineMetadata()

    # ========================================================================
    # Fluent API
    # ========================================================================

    def set_data_source(self, tag_name: str, days: int = 30) -> 'TrainingPipelineV2':
        """Îç∞Ïù¥ÌÑ∞ ÏÜåÏä§ ÏÑ§Ï†ï"""
        self._data_source = {'tag_name': tag_name, 'days': days}
        self.metadata.tag_name = tag_name
        return self

    def add_preprocessing(self) -> PreprocessingChain:
        """Ï†ÑÏ≤òÎ¶¨ Ï≤¥Ïù∏ ÏãúÏûë"""
        return PreprocessingChain(self)

    def add_feature_engineering(self) -> FeatureEngineeringChain:
        """ÌîºÏ≤ò ÏóîÏßÄÎãàÏñ¥ÎßÅ Ï≤¥Ïù∏ ÏãúÏûë"""
        return FeatureEngineeringChain(self)

    def add_model(self, model_type: str, **params) -> 'TrainingPipelineV2':
        """Î™®Îç∏ Ï∂îÍ∞Ä (Î†àÏßÄÏä§Ìä∏Î¶¨ÏóêÏÑú Í∞ÄÏ†∏Ïò§Í∏∞)"""
        plugin_class = ModelRegistry.get(model_type)
        plugin = plugin_class(**params)
        self._models.append(plugin)
        console.info(f"‚ûï Added model: {plugin.get_name()}")
        return self

    def add_validation(self, validation_type: str, **params) -> 'TrainingPipelineV2':
        """Í≤ÄÏ¶ù Ï†ÑÎûµ Ï∂îÍ∞Ä"""
        from ksys_app.ml.pipeline_builder import WalkForwardValidation

        if validation_type == 'walk_forward':
            self._validation = WalkForwardValidation(**params)
        return self

    def build(self) -> 'TrainingPipelineV2':
        """ÌååÏù¥ÌîÑÎùºÏù∏ ÎπåÎìú"""
        console.info("üî® Pipeline V2 built successfully")
        console.info(f"   Preprocessing: {len(self._preprocessing_steps)} steps")
        console.info(f"   Features: {len(self._feature_config)} types")
        console.info(f"   Models: {len(self._models)}")
        return self

    # ========================================================================
    # Execution
    # ========================================================================

    async def execute(self) -> Dict[str, Any]:
        """ÌååÏù¥ÌîÑÎùºÏù∏ Ïã§Ìñâ"""
        import time
        start_time = time.time()

        console.info("üöÄ Executing Pipeline V2...")

        # Step 1: Load data
        console.info("üì• Step 1: Loading data...")
        await self._load_data()

        # Step 2: Preprocessing
        if self._preprocessing_steps:
            console.info("üßπ Step 2: Preprocessing...")
            await self._apply_preprocessing()

        # Step 3: Feature engineering
        if self._feature_config:
            console.info("‚öôÔ∏è  Step 3: Feature engineering...")
            await self._apply_feature_engineering()

        # Step 4: Train models
        console.info("üéì Step 4: Training models...")
        results = {}

        for model in self._models:
            console.info(f"   Training {model.get_name()}...")

            # Train
            await model.train(self._processed_data)
            self.metadata.models_trained.append(model.get_name())

            # Validate
            if self._validation:
                metrics = await self._validation.validate(self._processed_data, model)
                results[model.get_name()] = {
                    'model': model,
                    'metrics': metrics
                }
                console.info(f"   ‚úÖ {model.get_name()} - MAPE: {metrics['mape']:.2f}%")

        # Find best model
        if results:
            best = min(results.items(), key=lambda x: x[1]['metrics'].get('mape', float('inf')))
            self.metadata.best_model = best[0]
            self.metadata.best_mape = best[1]['metrics']['mape']

        self.metadata.training_duration = time.time() - start_time

        console.info(f"‚úÖ Pipeline complete in {self.metadata.training_duration:.2f}s")
        console.info(f"   Best model: {self.metadata.best_model} (MAPE: {self.metadata.best_mape:.2f}%)")

        return {
            'results': results,
            'metadata': self.metadata
        }

    # ========================================================================
    # Internal Methods
    # ========================================================================

    async def _load_data(self):
        """Îç∞Ïù¥ÌÑ∞ Î°úÎìú - 10Î∂Ñ aggregation Îç∞Ïù¥ÌÑ∞Îßå ÏÇ¨Ïö©"""
        from sqlalchemy import text

        tag_name = self._data_source['tag_name']
        days = self._data_source['days']
        end_time = datetime.now(KST)
        start_time = end_time - timedelta(days=days)

        # ‚úÖ 10Î∂Ñ aggregation Îç∞Ïù¥ÌÑ∞Îßå ÏÇ¨Ïö© (ÏÑ±Îä• Î∞è Îç∞Ïù¥ÌÑ∞ ÌíàÏßà Ìñ•ÏÉÅ)
        query = text("""
            SELECT
                bucket AT TIME ZONE 'Asia/Seoul' as timestamp,
                avg as value
            FROM influx_agg_10m
            WHERE tag_name = :tag_name
              AND bucket >= :start_time
              AND bucket < :end_time
              AND avg IS NOT NULL
            ORDER BY bucket
        """)

        rows = (await self.session.execute(
            query,
            {'tag_name': tag_name, 'start_time': start_time, 'end_time': end_time}
        )).mappings().all()

        self._raw_data = pd.DataFrame([dict(r) for r in rows])
        self.metadata.raw_samples = len(self._raw_data)
        self.metadata.data_start = start_time
        self.metadata.data_end = end_time

        console.info(f"   Loaded {len(self._raw_data)} rows")

    async def _apply_preprocessing(self):
        """Ï†ÑÏ≤òÎ¶¨ Ï≤¥Ïù∏ Ïã§Ìñâ"""
        df = self._raw_data.copy()

        for step in self._preprocessing_steps:
            df = await step.apply(df, self.metadata)

        self._processed_data = df
        self.metadata.processed_samples = len(df)
        console.info(f"   Processed samples: {self.metadata.processed_samples}")

    async def _apply_feature_engineering(self):
        """ÌîºÏ≤ò ÏóîÏßÄÎãàÏñ¥ÎßÅ Ï†ÅÏö©"""
        df = self._processed_data if self._processed_data is not None else self._raw_data
        df = df.copy()

        self.metadata.original_features = len(df.columns)

        # Lag features
        if self._feature_config.get('lags'):
            for lag in self._feature_config['lags']:
                df[f'lag_{lag}h'] = df['value'].shift(lag)
                self.metadata.features_created.append(f'lag_{lag}h')

        # Rolling features
        if self._feature_config.get('rolling'):
            for window in self._feature_config['rolling']:
                df[f'rolling_mean_{window}h'] = df['value'].rolling(window).mean()
                df[f'rolling_std_{window}h'] = df['value'].rolling(window).std()
                self.metadata.features_created.extend([
                    f'rolling_mean_{window}h',
                    f'rolling_std_{window}h'
                ])

        # Temporal features
        if self._feature_config.get('temporal'):
            for comp in self._feature_config['temporal']:
                if comp == 'hour':
                    df['hour'] = df['timestamp'].dt.hour
                    self.metadata.features_created.append('hour')
                elif comp == 'dayofweek':
                    df['dayofweek'] = df['timestamp'].dt.dayofweek
                    self.metadata.features_created.append('dayofweek')

        df = df.dropna()
        self._processed_data = df
        self.metadata.final_features = len(df.columns)

        console.info(f"   Created {len(self.metadata.features_created)} features")


# ============================================================================
# Configuration-based Factory (Ï†úÏïàÌïòÏã† ÏÑ§Í≥Ñ)
# ============================================================================

async def create_pipeline_from_config(
    session: AsyncSession,
    config: Dict
) -> TrainingPipelineV2:
    """ÏÑ§Ï†ï Í∏∞Î∞ò ÌååÏù¥ÌîÑÎùºÏù∏ ÏÉùÏÑ±"""

    pipeline = TrainingPipelineV2(session)

    # Data source
    if 'data_source' in config:
        pipeline.set_data_source(**config['data_source'])

    # Preprocessing
    if 'preprocessing' in config:
        chain = pipeline.add_preprocessing()
        for step in config['preprocessing']:
            if step['type'] == 'interpolate':
                chain.interpolate(**step.get('params', {}))
            elif step['type'] == 'remove_outliers':
                chain.remove_outliers(**step.get('params', {}))
            elif step['type'] == 'scale':
                chain.scale(**step.get('params', {}))
        chain.done()

    # Feature engineering
    if 'feature_engineering' in config:
        chain = pipeline.add_feature_engineering()
        fe_config = config['feature_engineering']

        if 'lags' in fe_config:
            chain.add_lag(fe_config['lags'])
        if 'rolling' in fe_config:
            chain.add_rolling(fe_config['rolling'])
        if 'temporal' in fe_config:
            chain.add_temporal(fe_config['temporal'])

        chain.done()

    # Models
    if 'models' in config:
        for model_cfg in config['models']:
            pipeline.add_model(model_cfg['type'], **model_cfg.get('params', {}))

    # Validation
    if 'validation' in config:
        pipeline.add_validation(
            config['validation']['type'],
            **config['validation'].get('params', {})
        )

    return pipeline.build()
